mysub = which(ymd_hms(load_data$Time) %in% ymd_hms(rownames(temperature_impute)))
load_data = load_data[mysub,]
# De-duplicate the weather data by merging on first match of date in the load data
temp_ind = match(ymd_hms(load_data$Time), ymd_hms(rownames(temperature_impute)))
temperature_impute = temperature_impute[temp_ind,]
# Take the time stamps from the load data
time_stamp = ymd_hms(load_data$Time)
# Verify that the time stamps match row by row across the two data frames
all(time_stamp ==  ymd_hms(rownames(temperature_impute)))
# a lot of these station names are in Mexico or the Gulf
# and we don't have temperature data on them
station_data = subset(station_data, state != 'MX')
# Make a map.
# First, project project the lon, lat coordinates
# to the same coordinate system used by usmap
station_map = station_data %>%
select(lon, lat) %>%
usmap_transform
head(station_map)
# now merge these coordinates station name
station_data = station_data %>% rownames_to_column('station')
station_data = merge(station_data, station_map, by=c('lat', 'lon'))
head(station_data)
# plot the coordinates of the weather stations
plot_usmap(include = c("TX", "LA", "OK", "NM", "AR")) +
geom_point(data=station_data, aes(x=lon.1, y=lat.1))
library(tidyverse)
library(ggplot2)
library(usmap)
library(maptools)
library(lubridate)
library(randomForest)
library(splines)
library(pdp)
library(rgdal)
# Note: before loading the data,
# you'll first need to unzip the ercot folder
# (too big for GitHub if not compressed)
# Power grid load every hour for 6 1/2 years
# throughout the 8 ERCOT regions of Texas
# units of grid load are megawatts.
# This represents peak instantaneous demand for power in that hour.
# source: scraped from the ERCOT website
load_data = read.csv("../data/load_data.csv")
head(load_data)
# Now weather data at hundreds of weather stations
# throughout Texas and the surrounding region
# Note: I've imputed a handful of sporadic missing values
# Source: National Weather Service
temperature_impute = read.csv("../../data_workspace/temperature_impute.csv", row.names=1)
station_data = read.csv("../../data_workspace/station_data.csv", row.names=1)
# take a peak at the weather station data
head(temperature_impute)
head(station_data)
####
# Data cleaning
####
# some dates have completely missing weather data
# Keep the load data for dates when we have weather data
mysub = which(ymd_hms(load_data$Time) %in% ymd_hms(rownames(temperature_impute)))
load_data = load_data[mysub,]
# De-duplicate the weather data by merging on first match of date in the load data
temp_ind = match(ymd_hms(load_data$Time), ymd_hms(rownames(temperature_impute)))
temperature_impute = temperature_impute[temp_ind,]
# Take the time stamps from the load data
time_stamp = ymd_hms(load_data$Time)
# Verify that the time stamps match row by row across the two data frames
all(time_stamp ==  ymd_hms(rownames(temperature_impute)))
# a lot of these station names are in Mexico or the Gulf
# and we don't have temperature data on them
station_data = subset(station_data, state != 'MX')
# Make a map.
# First, project project the lon, lat coordinates
# to the same coordinate system used by usmap
station_map = station_data %>%
select(lon, lat) %>%
usmap_transform
head(station_map)
# now merge these coordinates station name
station_data = station_data %>% rownames_to_column('station')
station_data = merge(station_data, station_map, by=c('lat', 'lon'))
head(station_data)
# plot the coordinates of the weather stations
plot_usmap(include = c("TX", "LA", "OK", "NM", "AR")) +
geom_point(data=station_data, aes(x=lon, y=lat))
# Now run PCA on the weather data
pc_weather = prcomp(temperature_impute, rank=5, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_weather$rotation)
# plot the coordinates of the weather stations
plot_usmap(include = c("TX", "LA", "OK", "NM", "AR")) +
geom_point(data=station_data, aes(x=lon, y=lat))
station_data
# now merge these coordinates station name
station_data = station_data %>% rownames_to_column('station')
station_data = merge(station_data, station_map, by=c('lat', 'lon'))
head(station_data)
library(tidyverse)
library(ggplot2)
library(usmap)
library(maptools)
library(lubridate)
library(randomForest)
library(splines)
library(pdp)
library(rgdal)
# Note: before loading the data,
# you'll first need to unzip the ercot folder
# (too big for GitHub if not compressed)
# Power grid load every hour for 6 1/2 years
# throughout the 8 ERCOT regions of Texas
# units of grid load are megawatts.
# This represents peak instantaneous demand for power in that hour.
# source: scraped from the ERCOT website
load_data = read.csv("../data/load_data.csv")
head(load_data)
# Now weather data at hundreds of weather stations
# throughout Texas and the surrounding region
# Note: I've imputed a handful of sporadic missing values
# Source: National Weather Service
temperature_impute = read.csv("../../data_workspace/temperature_impute.csv", row.names=1)
station_data = read.csv("../../data_workspace/station_data.csv", row.names=1)
# take a peak at the weather station data
head(temperature_impute)
head(station_data)
####
# Data cleaning
####
# some dates have completely missing weather data
# Keep the load data for dates when we have weather data
mysub = which(ymd_hms(load_data$Time) %in% ymd_hms(rownames(temperature_impute)))
load_data = load_data[mysub,]
# De-duplicate the weather data by merging on first match of date in the load data
temp_ind = match(ymd_hms(load_data$Time), ymd_hms(rownames(temperature_impute)))
temperature_impute = temperature_impute[temp_ind,]
# Take the time stamps from the load data
time_stamp = ymd_hms(load_data$Time)
# Verify that the time stamps match row by row across the two data frames
all(time_stamp ==  ymd_hms(rownames(temperature_impute)))
# a lot of these station names are in Mexico or the Gulf
# and we don't have temperature data on them
station_data = subset(station_data, state != 'MX')
# Make a map.
# First, project project the lon, lat coordinates
# to the same coordinate system used by usmap
station_map = station_data %>%
select(lon, lat) %>%
usmap_transform
head(station_map)
# now merge these coordinates station name
station_data = station_data %>% rownames_to_column('station')
station_data = merge(station_data, station_map, by=c('lat', 'lon'))
head(station_data)
# plot the coordinates of the weather stations
plot_usmap(include = c("TX", "LA", "OK", "NM", "AR")) +
geom_point(data=station_data, aes(x=lon, y=lat))
setwd("~/Dropbox/SalemCenter/classes/PRL/Fall2022/Policy-Research-Laboratory/code")
groups = read.csv("../data/project_groups_2022.csv")
groups
groups = read.csv("../data/project_groups_2022.csv")
groups
groups = read.csv("../data/project_groups_2022.csv")
groups
groups = read.csv("../data/project_groups_2022.csv",header = F)
groups = read.csv("../data/project_groups_2022.csv",header = T)
groups
groups = read.csv("../data/project_groups_2022.csv",header = T)
groups
sample(1:nrow(groups))
set.seed(1)
groups = read.csv("../data/project_groups_2022.csv",header = T)
sample(1:nrow(groups))
set.seed(1)
groups = read.csv("../data/project_groups_2022.csv",header = T)
sample(1:nrow(groups))
set.seed(1)
groups = read.csv("../data/project_groups_2022.csv",header = T)
sample(1:nrow(groups))
ord = sample(1:nrow(groups))
set.seed(1)
groups = read.csv("../data/project_groups_2022.csv",header = T)
ord = sample(1:nrow(groups))
ord
groups[ord,]
cbind(groups[ord,],c('Tues','Tues','Tues','Thurs','Thurs'))
final = cbind(groups[ord,],c('Tues','Tues','Tues','Thurs','Thurs'))
colnames(final) = ''
final
final = cbind(,c('Tues','Tues','Tues','Thurs','Thurs'),groups[ord,])
final = cbind(c('Tues','Tues','Tues','Thurs','Thurs'),groups[ord,])
colnames(final) = ''
final
library(ggplot2)
library(ClusterR)  # for kmeans++
library(foreach)
library(mosaic)
cars = read.csv('../data/cars.csv', header=TRUE)
summary(cars)
heaD(ars)
head(cars)
X = cars[,-(1:9)]
X = scale(X, center=TRUE, scale=TRUE)
head(X)
colMeans(X)
round(colMeans(X))
apply(X,2,sd)
clust1 = kmeans(X, 6, nstart=500)
clust1$center  # not super helpful
sigma
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
sigma
mu
clust1$center[1,]*sigma + mu
clust1$center[4,]*sigma + mu
clust1$center[2,]*sigma + mu
which(clust1$cluster == 1)
which(clust1$cluster == 4)
qplot(Weight, Length, data=cars, color=factor(clust1$cluster))
which(clust1$cluster == 3)
qplot(Horsepower, CityMPG, data=cars, color=factor(clust1$cluster))
clust2 = KMeans_rcpp(X, clusters=6, num_init=25, initializer = 'kmeans++')
clust2$total_SSE
clust1$totss
clust2$clusters
library(tidyverse)
library(ggplot2)
library(ggcorrplot)
### data on tv shows from NBC market research
## show details; ratings and engagement
# https://digiday.com/marketing/what-is-a-grp-gross-ratings-point/
shows = read.csv("../data/nbc_showdetails.csv", row.names=1)
# predicted engagement versus gross ratings points
ggplot(shows) +
geom_point(aes(x=PE, y=GRP, color=Genre))
survey = read.csv("../data/nbc_pilotsurvey.csv")
head(survey)
pilot_results = survey %>%
group_by(Show) %>%
select(-Viewer) %>%
summarize_all(mean) %>%
column_to_rownames(var="Show")
head(pilot_results)
pilot_results = survey %>%
group_by(Show) %>%
select(-Viewer) %>%
summarize_all(mean) %>%
head(pilot_results)
pilot_results = survey %>%
group_by(Show) %>%
select(-Viewer) %>%
summarize_all(mean)
# now we have a tidy matrix of shows by questions
# each entry is an average survey response
head(pilot_results)
pilot_results = survey %>%
group_by(Show) %>%
select(-Viewer) %>%
summarize_all(mean) %>%
column_to_rownames(var="Show")
# now we have a tidy matrix of shows by questions
# each entry is an average survey response
head(pilot_results)
# a few quick plots
ggplot(rownames_to_column(pilot_results, "Show")) +
geom_col(aes(x=reorder(Show, -Q2_Relatable), y = Q2_Relatable)) +
coord_flip()
ggplot(rownames_to_column(pilot_results, "Show")) +
geom_col(aes(x=reorder(Show, -Q2_Confusing), y = Q2_Confusing)) +
coord_flip()
cor(pilot_results)
ggcorrplot::ggcorrplot(cor(pilot_results))
ggcorrplot::ggcorrplot(cor(pilot_results), hc.order = TRUE)
# Now look at PCA of the (average) survey responses.
# This is a common way to treat survey data
PCApilot = prcomp(pilot_results, scale=TRUE)
pilot_results
## variance plot
plot(PCApilot)
summary(PCApilot)
round(PCApilot$rotation[,1:3],2)
PCApilot$rotation[,1:3]
loadings_summary = PCApilot$rotation %>%
as.data.frame() %>%
rownames_to_column('Question')
loadings_summary %>%
select(Question, PC1) %>%
arrange(desc(PC1))
loadings_summary %>%
select(Question, PC2) %>%
arrange(desc(PC2))
loadings_summary %>%
select(Question, PC3) %>%
arrange(desc(PC3))
shows = merge(shows, PCApilot$x[,1:3], by="row.names")
shows = rename(shows, Show = Row.names)
shows
ggplot(shows) +
geom_col(aes(x=reorder(Show, PC1), y=PC1)) +
coord_flip()
loadings_summary %>%
select(Question, PC1) %>%
arrange(desc(PC1))
PCApilot$rotation[,1]
PCApilot$rotation[,1] %*% pilot_results[1,]
pilot_results[1,]
PCApilot$rotation[,1] %*% t(pilot_results[1,])
PCApilot$rotation[,1] %*% t(pilot_results[2,])
PCApilot$rotation[,1] %*% t(pilot_results[3,])
PCApilot$rotation[,2] %*% t(pilot_results[1,])
PCApilot
loadings_summary = PCApilot$rotation %>%
as.data.frame() %>%
rownames_to_column('Question')
shows = merge(shows, PCApilot$x[,1:3], by="row.names")
shows = rename(shows, Show = Row.names)
# looks like a "lighthearted vs serious" PC
ggplot(shows) +
geom_col(aes(x=reorder(Show, PC3), y=PC3)) +
coord_flip()
loadings_summary = PCApilot$rotation %>%
as.data.frame() %>%
rownames_to_column('Question')
# This seems to pick out characteristics of
# well-received dramas with positive loadings?
loadings_summary %>%
select(Question, PC1) %>%
arrange(desc(PC1))
# this just seems to load negatively on most things
# honestly not sure!
loadings_summary %>%
select(Question, PC2) %>%
arrange(desc(PC2))
# this looks clearly like a drama vs comedy axis
loadings_summary %>%
select(Question, PC3) %>%
arrange(desc(PC3))
ggplot(shows) +
geom_col(aes(x=reorder(Show, PC1), y=PC1)) +
coord_flip()
shows = merge(shows, PCApilot$x[,1:3], by="row.names")
shows = rename(shows, Show = Row.names)
library(tidyverse)
library(ggplot2)
library(ggcorrplot)
### data on tv shows from NBC market research
## show details; ratings and engagement
# https://digiday.com/marketing/what-is-a-grp-gross-ratings-point/
shows = read.csv("../data/nbc_showdetails.csv", row.names=1)
# predicted engagement versus gross ratings points
ggplot(shows) +
geom_point(aes(x=PE, y=GRP, color=Genre))
## Now read the pilot focus group survey results
## for each question, 1=strongly disagree, 5=strongly agree.
## 1: 'The show makes me feel ____', 2: 'I found the show ____'
survey = read.csv("../data/nbc_pilotsurvey.csv")
head(survey)
# there are lots of survey respondents
# let's calculate an average response for each show, for each question,
# across all respondents
pilot_results = survey %>%
group_by(Show) %>%
select(-Viewer) %>%
summarize_all(mean) %>%
column_to_rownames(var="Show")
# now we have a tidy matrix of shows by questions
# each entry is an average survey response
head(pilot_results)
# a few quick plots
ggplot(rownames_to_column(pilot_results, "Show")) +
geom_col(aes(x=reorder(Show, -Q2_Relatable), y = Q2_Relatable)) +
coord_flip()
ggplot(rownames_to_column(pilot_results, "Show")) +
geom_col(aes(x=reorder(Show, -Q2_Confusing), y = Q2_Confusing)) +
coord_flip()
# a look at the correlation matrix
cor(pilot_results)
# a quick heatmap visualization
ggcorrplot::ggcorrplot(cor(pilot_results))
# looks a mess -- reorder the variables by hierarchical clustering
ggcorrplot::ggcorrplot(cor(pilot_results), hc.order = TRUE)
# Now look at PCA of the (average) survey responses.
# This is a common way to treat survey data
PCApilot = prcomp(pilot_results, scale=TRUE)
## variance plot
plot(PCApilot)
summary(PCApilot)
# first few pcs
# try interpreting the loadings
# the question to ask is: "which variables does this load heavily on (positive and negatively)?"
round(PCApilot$rotation[,1:3],2)
# create a tidy summary of the loadings (column-centric view -- loadings, query vectors, etc.)
loadings_summary = PCApilot$rotation %>%
as.data.frame() %>%
rownames_to_column('Question')
# This seems to pick out characteristics of
# well-received dramas with positive loadings?
loadings_summary %>%
select(Question, PC1) %>%
arrange(desc(PC1))
# this just seems to load negatively on most things
# honestly not sure!
loadings_summary %>%
select(Question, PC2) %>%
arrange(desc(PC2))
# this looks clearly like a drama vs comedy axis
loadings_summary %>%
select(Question, PC3) %>%
arrange(desc(PC3))
# Let's make some plots of the shows themselves in
# PC space, i.e. the space of summary variables we've created
shows = merge(shows, PCApilot$x[,1:3], by="row.names")
shows = rename(shows, Show = Row.names)
shows = merge(shows, PCApilot$x[,1:3], by="row.names")
shows = rename(shows, Show = Row.names)
library(tidyverse)
library(ggplot2)
library(ggcorrplot)
### data on tv shows from NBC market research
## show details; ratings and engagement
# https://digiday.com/marketing/what-is-a-grp-gross-ratings-point/
shows = read.csv("../data/nbc_showdetails.csv", row.names=1)
# predicted engagement versus gross ratings points
ggplot(shows) +
geom_point(aes(x=PE, y=GRP, color=Genre))
## Now read the pilot focus group survey results
## for each question, 1=strongly disagree, 5=strongly agree.
## 1: 'The show makes me feel ____', 2: 'I found the show ____'
survey = read.csv("../data/nbc_pilotsurvey.csv")
head(survey)
# there are lots of survey respondents
# let's calculate an average response for each show, for each question,
# across all respondents
pilot_results = survey %>%
group_by(Show) %>%
select(-Viewer) %>%
summarize_all(mean) %>%
column_to_rownames(var="Show")
# now we have a tidy matrix of shows by questions
# each entry is an average survey response
head(pilot_results)
# a few quick plots
ggplot(rownames_to_column(pilot_results, "Show")) +
geom_col(aes(x=reorder(Show, -Q2_Relatable), y = Q2_Relatable)) +
coord_flip()
ggplot(rownames_to_column(pilot_results, "Show")) +
geom_col(aes(x=reorder(Show, -Q2_Confusing), y = Q2_Confusing)) +
coord_flip()
# a look at the correlation matrix
cor(pilot_results)
# a quick heatmap visualization
ggcorrplot::ggcorrplot(cor(pilot_results))
# looks a mess -- reorder the variables by hierarchical clustering
ggcorrplot::ggcorrplot(cor(pilot_results), hc.order = TRUE)
# Now look at PCA of the (average) survey responses.
# This is a common way to treat survey data
PCApilot = prcomp(pilot_results, scale=TRUE)
## variance plot
plot(PCApilot)
summary(PCApilot)
# first few pcs
# try interpreting the loadings
# the question to ask is: "which variables does this load heavily on (positive and negatively)?"
round(PCApilot$rotation[,1:3],2)
# create a tidy summary of the loadings (column-centric view -- loadings, query vectors, etc.)
loadings_summary = PCApilot$rotation %>%
as.data.frame() %>%
rownames_to_column('Question')
# This seems to pick out characteristics of
# well-received dramas with positive loadings?
loadings_summary %>%
select(Question, PC1) %>%
arrange(desc(PC1))
# this just seems to load negatively on most things
# honestly not sure!
loadings_summary %>%
select(Question, PC2) %>%
arrange(desc(PC2))
# this looks clearly like a drama vs comedy axis
loadings_summary %>%
select(Question, PC3) %>%
arrange(desc(PC3))
# Let's make some plots of the shows themselves in
# PC space, i.e. the space of summary variables we've created
shows = merge(shows, PCApilot$x[,1:3], by="row.names")
shows = rename(shows, Show = Row.names)
ggplot(shows) +
geom_col(aes(x=reorder(Show, PC1), y=PC1)) +
coord_flip()
ggplot(shows) +
geom_col(aes(x=reorder(Show, PC3), y=PC3)) +
coord_flip()
# principal component regression: predicted engagement
lm1 = lm(PE ~ PC1 + PC2 + PC3, data=shows)
summary(lm1)
# Protein first
protein = read.csv("../data/protein.csv", row.names=1)
# Center/scale the data
protein_scaled = scale(protein, center=TRUE, scale=TRUE)
# Form a pairwise distance matrix using the dist function
protein_distance_matrix = dist(protein_scaled, method='euclidean')
# Now run hierarchical clustering
hier_protein = hclust(protein_distance_matrix, method='average')
# Plot the dendrogram
plot(hier_protein, cex=0.8)
cluster1 = cutree(hier_protein, k=5)
summary(factor(cluster1))
which(cluster1 == 1)
which(cluster1 == 2)
which(cluster1 == 3)
