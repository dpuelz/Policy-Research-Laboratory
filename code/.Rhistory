#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# the number at the top is the variance of the projected points
hist(alpha, 25, xlim=c(-3,3), main=round(var(alpha), 2))
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# the number at the top is the variance of the projected points
hist(alpha, 25, xlim=c(-3,3), main=round(var(alpha), 2))
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# the number at the top is the variance of the projected points
hist(alpha, 25, xlim=c(-3,3), main=round(var(alpha), 2))
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# the number at the top is the variance of the projected points
hist(alpha, 25, xlim=c(-3,3), main=round(var(alpha), 2))
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# the number at the top is the variance of the projected points
hist(alpha, 25, xlim=c(-3,3), main=round(var(alpha), 2))
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# the number at the top is the variance of the projected points
hist(alpha, 25, xlim=c(-3,3), main=round(var(alpha), 2))
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# the number at the top is the variance of the projected points
hist(alpha, 25, xlim=c(-3,3), main=round(var(alpha), 2))
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# the number at the top is the variance of the projected points
hist(alpha, 25, xlim=c(-3,3), main=round(var(alpha), 2))
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# the number at the top is the variance of the projected points
hist(alpha, 25, xlim=c(-3,3), main=round(var(alpha), 2))
#### begin block
v_try = rnorm(2)
v_try = v_try/sqrt(sum(v_try^2))  # normalize to unit length
par(mfrow=c(1,2))
plot(Z, pch=19, col=rgb(0.3,0.3,0.3,0.3),
xlim=c(-2.5,2.5), ylim=c(-2.5,2.5))
slope = v_try[2]/v_try[1]
abline(0, slope)  # plot the subspace as a line
# Project the points onto that subspace
alpha = Z %*% v_try  # inner product of each row with v_try
z_hat = alpha %*% v_try  # locations in R^2
points(z_hat, col='blue', pch=4)
segments(0, 0, v_try[1], v_try[2], col='red', lwd=4)
# the number at the top is the variance of the projected points
hist(alpha, 25, xlim=c(-3,3), main=round(var(alpha), 2))
pc_Z = prcomp(Z, rank=1)
pc_Z$rotation
v_try
pc_Z$x
summary(pc_Z)
library(tidyverse)
library(ggplot2)
library(usmap)
library(lubridate)
library(randomForest)
library(splines)
library(pdp)
# Note: before loading the data,
# you'll first need to unzip the ercot folder
# (too big for GitHub if not compressed)
# Power grid load every hour for 6 1/2 years
# throughout the 8 ERCOT regions of Texas
# units of grid load are megawatts.
# This represents peak instantaneous demand for power in that hour.
# source: scraped from the ERCOT website
load_data = read.csv("../data/ercot/load_data.csv")
head(load_data)
# Now weather data at hundreds of weather stations
# throughout Texas and the surrounding region
# Note: I've imputed a handful of sporadic missing values
# Source: National Weather Service
temperature_impute = read.csv("../data/ercot/temperature_impute.csv", row.names=1)
station_data = read.csv("../data/ercot/station_data.csv", row.names=1)
# take a peak at the weather station data
head(temperature_impute)
head(station_data)
mysub = which(ymd_hms(load_data$Time) %in% ymd_hms(rownames(temperature_impute)))
load_data = load_data[mysub,]
# De-duplicate the weather data by merging on first match of date in the load data
temp_ind = match(ymd_hms(load_data$Time), ymd_hms(rownames(temperature_impute)))
temperature_impute = temperature_impute[temp_ind,]
# Take the time stamps from the load data
time_stamp = ymd_hms(load_data$Time)
# Verify that the time stamps match row by row across the two data frames
all(time_stamp ==  ymd_hms(rownames(temperature_impute)))
# a lot of these station names are in Mexico or the Gulf
# and we don't have temperature data on them
station_data = subset(station_data, state != 'MX')
View(temperature_impute)
station_map = station_data %>%
select(lon, lat) %>%
usmap_transform
head(station_map)
# now merge these coordinates station name
station_data = station_data %>% rownames_to_column('station')
station_data = merge(station_data, station_map, by=c('lat', 'lon'))
head(station_data)
# plot the coordinates of the weather stations
plot_usmap(include = c("TX", "LA", "OK", "NM", "AR")) +
geom_point(data=station_data, aes(x=x, y=y))
# Now run PCA on the weather data
pc_weather = prcomp(temperature_impute, rank=5, scale=TRUE)
head(pc_weather$rotation)
summary(pc_weather)
loadings = pc_weather$rotation %>%
as.data.frame %>%
rownames_to_column('station')
station_data = merge(station_data, loadings, by = 'station')
head(station_data)
# set up the map and the color scale
p0 = plot_usmap(include = c("TX", "LA", "OK", "NM", "AR")) +
scale_color_gradient(low = 'blue', high='red')
p0 + geom_point(data=station_data, aes(x=x, y=y, color=PC1))
p0 + geom_point(data=station_data, aes(x=x, y=y, color=PC2))
p0 + geom_point(data=station_data, aes(x=x, y=y, color=PC3))
pc_Z$rotation
p0 + geom_point(data=station_data, aes(x=x, y=y, color=PC4))
p0 + geom_point(data=station_data, aes(x=x, y=y, color=PC5))
p0 + geom_point(data=station_data, aes(x=x, y=y, color=PC1))
# clearly contrasting temperature along the coast versus everywhere else
# probably useful for predicting power load in Houston.
# the corresponding score is positive whenever the coast is warmer than
# the rest of texas, relatively speaking (i.e "hot for houston" and "cool for Amarillo")
p0 + geom_point(data=station_data, aes(x=x, y=y, color=PC2))
scores = pc_weather$x
p1 = pc_weather$x %>%
as.data.frame %>%
rownames_to_column('time') %>%
mutate(time = ymd_hms(time)) %>%
ggplot
p1 + geom_line(aes(x=time, y=PC1))
p1 + geom_line(aes(x=yday(time), y=PC1)) + facet_wrap(~year(time))
# PC2 score over time
# Not nearly so periodic
p1 + geom_line(aes(x=time, y=PC2))
p1 + geom_line(aes(x=yday(time), y=PC2)) + facet_wrap(~year(time))
p0 + geom_point(data=station_data, aes(x=x, y=y, color=PC2))
p1 + geom_line(aes(x=time, y=PC2))
p1 + geom_line(aes(x=yday(time), y=PC2)) + facet_wrap(~year(time))
dev.off()
rm(list=ls())
library(mosaic)
##### Working with on original Levitt data
#' The response variable, Y , is per capita crime rates (violent crime, property crime, and murders) by state, from 1985 to 1997 (inclusive). The treatment variable, Z, is the “effective” abortion rate. This metric is an averaged abortion rate, weighted by criminal age at the time of arrest (to account for the fact that crimes committed by criminals should be associated with abortion rates at the time of their births).
##########################################
Original = read.table("../data/levitt_ex.dat",fill=TRUE,header=TRUE)
setwd("~/Dropbox/SalemCenter/classes/PRL/Fall2023/Policy-Research-Laboratory/code")
rm(list=ls())
library(mosaic)
##### Working with on original Levitt data
#' The response variable, Y , is per capita crime rates (violent crime, property crime, and murders) by state, from 1985 to 1997 (inclusive). The treatment variable, Z, is the “effective” abortion rate. This metric is an averaged abortion rate, weighted by criminal age at the time of arrest (to account for the fact that crimes committed by criminals should be associated with abortion rates at the time of their births).
##########################################
Original = read.table("../data/levitt_ex.dat",fill=TRUE,header=TRUE)
n=dim(Original)[1]
## Remove DC, Alaska and Hawaii
ind1 = (1:n)[Original$statenum==9]
ind2 = (1:n)[Original$statenum==2]
ind3 = (1:n)[Original$statenum==12]
ind = c(ind1,ind2,ind3)
ind1 = (1:n)[Original$year>97]
ind2 = (1:n)[Original$year<85]
ind = c(ind,ind1,ind2)
ind = unique(ind)
ind = complete.cases(Data)
setwd("~/Dropbox/SalemCenter/classes/PRL/Fall2023/Policy-Research-Laboratory/code")
rm(list=ls())
library(mosaic)
##### Working with on original Levitt data
#' The response variable, Y , is per capita crime rates (violent crime, property crime, and murders) by state, from 1985 to 1997 (inclusive). The treatment variable, Z, is the “effective” abortion rate. This metric is an averaged abortion rate, weighted by criminal age at the time of arrest (to account for the fact that crimes committed by criminals should be associated with abortion rates at the time of their births).
##########################################
Original = read.table("../data/levitt_ex.dat",fill=TRUE,header=TRUE)
n=dim(Original)[1]
## Remove DC, Alaska and Hawaii
ind1 = (1:n)[Original$statenum==9]
ind2 = (1:n)[Original$statenum==2]
ind3 = (1:n)[Original$statenum==12]
ind = c(ind1,ind2,ind3)
ind1 = (1:n)[Original$year>97]
ind2 = (1:n)[Original$year<85]
ind = c(ind,ind1,ind2)
ind = unique(ind)
Data = Original[-ind,]
ind = complete.cases(Data)
Data = Data[ind,]
Data[,2] = Data[,2]-84
###
state.f = factor(Data$statenum)
states_dummies = model.matrix(~state.f)
year.f = factor(Data$year)
year_dummies = model.matrix(~year.f)
Y_M = Data$lpc_murd
D_M = Data$efamurd
Y_P = Data$lpc_prop
D_P = Data$efaprop
Y_V = Data$lpc_viol
D_V = Data$efaviol
Controls = Data[,-c(1:9)]
year = Data[,2]
Interactions = Controls*as.numeric(year)
Interactions2 = Controls*(as.numeric(year)^2)
Interactions3 = states_dummies*as.numeric(year)
Interactions4 = states_dummies*as.numeric(year^2)
XO = as.matrix(cbind(Controls,states_dummies[,2:48],year_dummies[,2:13]))
XO = scale(XO)
X = as.matrix(cbind(Controls,states_dummies[,2:48],year_dummies[,2:13],Interactions,Interactions2,Interactions3[,2:48],Interactions4[,2:48]))
X = scale(X)
X = X[,-71]
confint(lm(Y_V~D_V+XO))[2,]
confint(lm(Y_P~D_P+XO))[2,]
confint(lm(Y_M~D_M+XO))[2,]
dim(X)
dim(X0)
dim(XO)
rm(list=ls())
library(mosaic)
##### Working with on original Levitt data
#' The response variable, Y , is per capita crime rates (violent crime, property crime, and murders) by state, from 1985 to 1997 (inclusive). The treatment variable, Z, is the “effective” abortion rate. This metric is an averaged abortion rate, weighted by criminal age at the time of arrest (to account for the fact that crimes committed by criminals should be associated with abortion rates at the time of their births).
##########################################
Original = read.table("../data/levitt_ex.dat",fill=TRUE,header=TRUE)
n=dim(Original)[1]
## Remove DC, Alaska and Hawaii
ind1 = (1:n)[Original$statenum==9]
ind2 = (1:n)[Original$statenum==2]
ind3 = (1:n)[Original$statenum==12]
ind = c(ind1,ind2,ind3)
ind1 = (1:n)[Original$year>97]
ind2 = (1:n)[Original$year<85]
ind = c(ind,ind1,ind2)
ind = unique(ind)
Data = Original[-ind,]
ind = complete.cases(Data)
Data = Data[ind,]
Data[,2] = Data[,2]-84
###
state.f = factor(Data$statenum)
states_dummies = model.matrix(~state.f)
year.f = factor(Data$year)
year_dummies = model.matrix(~year.f)
Y_M = Data$lpc_murd
D_M = Data$efamurd
Y_P = Data$lpc_prop
D_P = Data$efaprop
Y_V = Data$lpc_viol
D_V = Data$efaviol
Controls = Data[,-c(1:9)]
year = Data[,2]
Interactions = Controls*as.numeric(year)
Interactions2 = Controls*(as.numeric(year)^2)
Interactions3 = states_dummies*as.numeric(year)
Interactions4 = states_dummies*as.numeric(year^2)
XO = as.matrix(cbind(Controls,states_dummies[,2:48],year_dummies[,2:13]))
XO = scale(XO)
X = as.matrix(cbind(Controls,states_dummies[,2:48],year_dummies[,2:13],Interactions,Interactions2,Interactions3[,2:48],Interactions4[,2:48]))
X = scale(X)
X = X[,-71]
View(XO)
confint(lm(Y_V~D_V+XO))[2,]
# OLS confidence intervals for the causal effect (small model)
confint(lm(Y_V~D_V+XO))[2,]
confint(lm(Y_P~D_P+XO))[2,]
confint(lm(Y_M~D_M+XO))[2,]
dim(X)
dim(XO)
CE_V_OLS = confint(lm(Y_V~D_V+X))[2,]
# OLS confidence intervals for the causal effect (BIG model)
CE_V_OLS = confint(lm(Y_V~D_V+X))[2,]
CE_P_OLS = confint(lm(Y_P~D_P+X))[2,]
CE_M_OLS = confint(lm(Y_M~D_M+X))[2,]
CE_V_OLS
CE_P_OLS
CE_M_OLS
CE_V_OLSsmallmodel = confint(lm(Y_V~D_V+XO))[2,]
## lasso-based selection (naive - large model)
library(glmnet)
boot = do(100)*{
rowindices = 1:nrow(X)
iixx = sample(rowindices,size=length(rowindices),replace=TRUE)
Xmod = X[iixx,]; Dmod = D_P[iixx]; Ymod = Y_V[iixx]
# fitting the model!
fit_reg = cv.glmnet(x=cbind(Dmod,Xmod),y=Ymod,family="gaussian",alpha=0,penalty.factor	=c(0,rep(1,ncol(Xmod))),nlambda=20)
as.numeric(coef(fit_reg,s="lambda.1se")[2])
}
causal_effect = unlist(boot)
hist(causal_effect)
hist(causal_effect)
CE_V_naive = confint(causal_effect,level = 0.95)
library(glmnet)
boot = do(100)*{
rowindices = 1:nrow(X)
iixx = sample(rowindices,size=length(rowindices),replace=TRUE)
Xmod = X[iixx,]; Dmod = D_P[iixx]; Ymod = Y_V[iixx]
# fitting the treatment model
fit_D = cv.glmnet(x=Xmod,y=Dmod,family="gaussian",alpha=0,nlambda=20)
Dhat = predict(fit_D,newx=Xmod,s="lambda.1se")
# fitting the outcome model
fit_reg = cv.glmnet(x=cbind(Dmod-Dhat,Xmod),y=Ymod,family="gaussian",alpha=0,penalty.factor	=c(0,rep(1,ncol(Xmod))),nlambda=20)
as.numeric(coef(fit_reg,s="lambda.1se")[2])
}
causal_effect = unlist(boot)
CE_V_best = confint(causal_effect,level = 0.95)
# plot to visualize
yval = c(1,1,2,2,3,3,4,4)
xval = unlist(c(CE_V_OLSsmallmodel,CE_V_OLS,CE_V_naive,CE_V_best))
plot(xval,yval,col='white',bty='n',xlab='causal effect',main='Effect of abortion rate on violent crime')
legend('topleft',legend=c('OLS-small','OLS-large','Reg-naive','Reg-best'),col=c('red','blue','orange','green'),lwd=4)
abline(v=0,lty=2)
plot(xval,yval,col='white',bty='n',xlab='causal effect',main='Effect of abortion rate on violent crime')
legend('topleft',legend=c('OLS-small','OLS-large','Reg-naive','Reg-best'),col=c('red','blue','orange','green'),lwd=4)
abline(v=0,lty=2)
plot(xval,yval,col='white',bty='n',xlab='causal effect',main='Effect of abortion rate on violent crime')
legend('topleft',legend=c('OLS-small','OLS-large','Reg-naive','Reg-best'),col=c('red','blue','orange','green'),lwd=4)
abline(v=0,lty=2)
lines(xval[1:2],yval[1:2],lwd=10,col='red')
lines(xval[3:4],yval[3:4],lwd=10,col='blue')
lines(xval[5:6],yval[5:6],lwd=10,col='orange')
lines(xval[7:8],yval[7:8],lwd=10,col='green')
