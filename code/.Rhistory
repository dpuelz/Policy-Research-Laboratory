# is that there's not enough data to estimate a separate interaction term for that item...
# and so the model defaults/shrinks to a "global" elasticity for that item.
coef(ml1)
# how can I get the elasticities?
price_main = coef(ml1)[2]
which_int = grep("log(price):item", rownames(coef(ml1)), fixed=TRUE)
price_int = coef(ml1)[which_int]
# these look much more reasonable, though not all negative.
# and the elephant in the room: of course price is not exogenous here!
# price is changing over time and in response to features that also predict demand.
hist(price_main + price_int)
####
# Lets orthogonalize instead.
# strategy: isolate "idiosyncratic" variation price and quantity sold
# by first explicitly adjusting for item and week.
####
# orthogonalization steps 1-2
xitem = sparse.model.matrix(~item-1, lmr=1e-5, data=beer)
xweek = sparse.model.matrix(~week-1, lmr=1e-5, data=beer)
xx = cbind(xweek, xitem)
# isolate variation in log(price) predicted by item and week
# interpretation: this model tells us what variation in pricing strategies
# seems to be predictable across items and weeks.
# The leftover, residual variation might plausibly be interpreted
# as "pseudo-experimental" variation -- i.e. random shocks to price.
# we'll use these random shocks to identify an elasticity.
pfit = gamlr(x=xx, y=log(beer$price), lmr=1e-5, standardize=FALSE)
# and now we isolate variation in quantity sold predicted by item and week.
# the residuals from this model are our "independent signal", i.e. variation in
# sales that might be driven _uniquely_ by price.
qfit = gamlr(x=xx, y=log(beer$units), lmr=1e-5, standardize=FALSE)
# Calculate residuals: variation in price and units sold that
# cannot be predicted by item and week
lpr = drop(log(beer$price) - predict(pfit, xx))
lqr = drop(log(beer$units) - predict(qfit, xx))
# Run 3rd ML step to get elasticities
# Now here's where text-mining comes in!
# Let's parse the item description text
# so that each individual word in the description becomes a predictor.
# so, e.g "IPA" changes the elasticity, "Lite" changes the elasticity, etc.
# and we can estimate these common effects by pooling information across all beers.
# let's create a doc-term matrix from the item descriptions
library(tm)
descr = Corpus(VectorSource(as.character(beer$description)))
xtext = DocumentTermMatrix(descr)
# convert to Matrix format
xtext = sparseMatrix(i=xtext$i,j=xtext$j, x=as.numeric(xtext$v>0),
dims=dim(xtext), dimnames=dimnames(xtext))
colnames(xtext)
# fit a model including interactions between text features and log price residuals
xtreat = cbind(1,xtext)
ofit = gamlr(x=lpr*xtreat, y=lqr, standardize=FALSE, free=1)
# gams represents the changes in elasticity associated with each term
# remember: elasticity is a negative number!
# so positive changes to elasticity mean that consumers are less price sensitive.
# negative changes to elasticity mean that consumers are more price sensitive.
gams = coef(ofit)[-1,]
# these terms are associated with higher price sensitivity
gams %>% sort %>% head(10)
# these terms are associated with lower price sensitivty
gams %>% sort %>% tail(10)
# create a query matrix, matching each level to a row in X
test_ind = match(levels(beer$item),beer$item)
xtest = xtext[test_ind,]
rownames(xtest) = beer$description[test_ind]
# translate into elasticities and plot
# these look very reasonable! most between -1 and -4.
# no/few spuriously positive estimates.
el = drop(gams[1] + xtest%*%gams[(1:ncol(xtext))+1])
hist(el, xlab="OML elasticities", xlim=c(-6,1), col="lightblue", main="", breaks=seq(-11, 1, by=0.2))
dev.off()
library(Matrix)
library(slam)
library(gamlr)
## small beer dataset
beer = read.csv("../data/smallbeer.csv",
colClasses=c(rep("factor",3),rep("numeric",2)))
# we have an item code, a natural language description, a week,
# and the price/quantity sold in that week for each item
head(beer)
nrow(beer)
oneforall = lm(log(units) ~ log(price)*item, data=beer)
# tons of NAs, lots of noisy coefficients
coef(oneforall)
hist(coef(oneforall)) ## super noisy zeros
price_main = coef(oneforall)[2]
which_int = grep("log(price):item", names(coef(oneforall)), fixed=TRUE)
price_int = coef(oneforall)[which_int]
# a histogram of the item-level elasticities
hist(price_main + price_int)
library(gamlr)
# This regression design adjusts for week by
# including dummy variables for each week.
# designed to soak up some of the variation in sales due to
# seasonal changes in beer demand
x1 = sparse.model.matrix(~log(price)*item + factor(week)-1, data=beer)
head(x1)
dim(x1)
ml1 = cv.gamlr(x=x1, y=log(beer$units), free = 1, standardize=FALSE, verb=TRUE)
coef(ml1)
# how can I get the elasticities?
price_main = coef(ml1)[2]
which_int = grep("log(price):item", rownames(coef(ml1)), fixed=TRUE)
price_int = coef(ml1)[which_int]
hist(price_main + price_int)
# orthogonalization steps 1-2
xitem = sparse.model.matrix(~item-1, lmr=1e-5, data=beer)
xweek = sparse.model.matrix(~week-1, lmr=1e-5, data=beer)
xx = cbind(xweek, xitem)
dim(xx)
pfit = gamlr(x=xx, y=log(beer$price), lmr=1e-5, standardize=FALSE)
qfit = gamlr(x=xx, y=log(beer$units), lmr=1e-5, standardize=FALSE)
# Calculate residuals: variation in price and units sold that
# cannot be predicted by item and week
lpr = drop(log(beer$price) - predict(pfit, xx))
lqr = drop(log(beer$units) - predict(qfit, xx))
library(tm)
descr = Corpus(VectorSource(as.character(beer$description)))
xtext = DocumentTermMatrix(descr)
beer$description
xtext = DocumentTermMatrix(descr)
dim(xtext)
# convert to Matrix format
xtext = sparseMatrix(i=xtext$i,j=xtext$j, x=as.numeric(xtext$v>0),
dims=dim(xtext), dimnames=dimnames(xtext))
colnames(xtext)
xtreat = cbind(1,xtext)
ofit = gamlr(x=lpr*xtreat, y=lqr, standardize=FALSE, free=1)
gams = coef(ofit)[-1,]
gams %>% sort %>% head(10)
gams %>% sort %>% tail(10)
test_ind = match(levels(beer$item),beer$item)
xtest = xtext[test_ind,]
rownames(xtest) = beer$description[test_ind]
el = drop(gams[1] + xtest%*%gams[(1:ncol(xtext))+1])
hist(el, xlab="OML elasticities", xlim=c(-6,1), col="lightblue", main="", breaks=seq(-11, 1, by=0.2))
sort(el) %>% head(20)  # some major brands, and some "faux craft"
sort(el) %>% tail(20)  # a lot of ciders and large-format (e.g. 24
rm(list=ls())
library(mosaic)
##### Working with on original Levitt data
#' The response variable, Y , is per capita crime rates (violent crime, property crime, and murders) by state, from 1985 to 1997 (inclusive). The treatment variable, Z, is the “effective” abortion rate. This metric is an averaged abortion rate, weighted by criminal age at the time of arrest (to account for the fact that crimes committed by criminals should be associated with abortion rates at the time of their births).
##########################################
Original = read.table("../data/levitt_ex.dat",fill=TRUE,header=TRUE)
n=dim(Original)[1]
set.seed(301)
f=function(x){
ifelse(x<.2,pnorm(x),-2*x^2+2*x)
}
curve(f,-1,1)
n = 200
xx = runif(n,-1,1)
yy = f(xx) + rnorm(n,0,.04)
points(xx,yy,pch=19,cex=.1)
x.test = seq(-1,1,length.out = 2000)
library(BART)
model = wbart(x.train = xx,y.train = yy,x.test = x.test,ntree=200)
matplot(x.test,t(model$yhat.test),type='l',col=rgb(1,0,0,alpha=0.05),lty=1)
points(xx,yy,pch=19,cex=.8,col=rgb(0,0,0,alpha=0.5),xlab="X",ylab='Y')
# curve(f,-1,1,add=T,col='purple',lwd=1.5)
lines(x.test,model$yhat.test.mean,col=1,type='l',lwd=2.5)
lines(x.test,apply(model$yhat.test,2,quantile,.975),col=1,type='l',lwd=1)
lines(x.test,apply(model$yhat.test,2,quantile,.025),col=1,type='l',lwd=1)
set.seed(301)
f=function(x){
ifelse(x<.2,pnorm(x),-2*x^2+2*x)
}
curve(f,-1,1)
n = 200
xx = runif(n,-1,1)
yy = f(xx) + rnorm(n,0,.04)
points(xx,yy,pch=19,cex=.1)
x.test = seq(-1,1,length.out = 2000)
library(BART)
model = wbart(x.train = xx,y.train = yy,x.test = x.test,ntree=200)
matplot(x.test,t(model$yhat.test),type='l',col=rgb(1,0,0,alpha=0.05),lty=1)
points(xx,yy,pch=19,cex=.8,col=rgb(0,0,0,alpha=0.5),xlab="X",ylab='Y')
lines(x.test,model$yhat.test.mean,col=1,type='l',lwd=2.5)
lines(x.test,apply(model$yhat.test,2,quantile,.975),col=1,type='l',lwd=1)
lines(x.test,apply(model$yhat.test,2,quantile,.025),col=1,type='l',lwd=1)
model = wbart(x.train = xx,y.train = yy,x.test = x.test,ntree=400)
matplot(x.test,t(model$yhat.test),type='l',col=rgb(1,0,0,alpha=0.05),lty=1)
points(xx,yy,pch=19,cex=.8,col=rgb(0,0,0,alpha=0.5),xlab="X",ylab='Y')
# curve(f,-1,1,add=T,col='purple',lwd=1.5)
lines(x.test,model$yhat.test.mean,col=1,type='l',lwd=2.5)
lines(x.test,apply(model$yhat.test,2,quantile,.975),col=1,type='l',lwd=1)
lines(x.test,apply(model$yhat.test,2,quantile,.025),col=1,type='l',lwd=1)
source("~/Dropbox/SalemCenter/classes/PRL/Fall2022/Policy-Research-Laboratory/code/makeBARTsim.R", echo=TRUE)
x.test = seq(-1,1,length.out = 5000)
library(BART)
model = wbart(x.train = xx,y.train = yy,x.test = x.test,ntree=200)
matplot(x.test,t(model$yhat.test),type='l',col=rgb(1,0,0,alpha=0.05),lty=1)
points(xx,yy,pch=19,cex=.8,col=rgb(0,0,0,alpha=0.5),xlab="X",ylab='Y')
# curve(f,-1,1,add=T,col='purple',lwd=1.5)
lines(x.test,model$yhat.test.mean,col=1,type='l',lwd=2.5)
lines(x.test,apply(model$yhat.test,2,quantile,.975),col=1,type='l',lwd=1)
lines(x.test,apply(model$yhat.test,2,quantile,.025),col=1,type='l',lwd=1)
set.seed(301)
f=function(x){
ifelse(x<.2,pnorm(x),-2*x^2+2*x)
}
curve(f,-1,1)
n = 200
xx = runif(n,-1,1)
yy = f(xx) + rnorm(n,0,.04)
points(xx,yy,pch=19,cex=.1)
x.test = seq(-1,1,length.out = 1000)
set.seed(301)
f=function(x){
ifelse(x<.2,pnorm(x),-2*x^2+2*x)
}
curve(f,-1,1)
n = 500
xx = runif(n,-1,1)
yy = f(xx) + rnorm(n,0,.04)
points(xx,yy,pch=19,cex=.1)
x.test = seq(-1,1,length.out = 1000)
library(BART)
model = wbart(x.train = xx,y.train = yy,x.test = x.test,ntree=200)
matplot(x.test,t(model$yhat.test),type='l',col=rgb(1,0,0,alpha=0.05),lty=1)
points(xx,yy,pch=19,cex=.8,col=rgb(0,0,0,alpha=0.5),xlab="X",ylab='Y')
# curve(f,-1,1,add=T,col='purple',lwd=1.5)
lines(x.test,model$yhat.test.mean,col=1,type='l',lwd=2.5)
lines(x.test,apply(model$yhat.test,2,quantile,.975),col=1,type='l',lwd=1)
lines(x.test,apply(model$yhat.test,2,quantile,.025),col=1,type='l',lwd=1)
set.seed(301)
f=function(x){
ifelse(x<.2,pnorm(x),-2*x^2+2*x)
}
curve(f,-1,1)
n = 100
xx = runif(n,-1,1)
yy = f(xx) + rnorm(n,0,.04)
points(xx,yy,pch=19,cex=.1)
x.test = seq(-1,1,length.out = 1000)
library(BART)
model = wbart(x.train = xx,y.train = yy,x.test = x.test,ntree=200)
matplot(x.test,t(model$yhat.test),type='l',col=rgb(1,0,0,alpha=0.05),lty=1)
points(xx,yy,pch=19,cex=.8,col=rgb(0,0,0,alpha=0.5),xlab="X",ylab='Y')
# curve(f,-1,1,add=T,col='purple',lwd=1.5)
lines(x.test,model$yhat.test.mean,col=1,type='l',lwd=2.5)
lines(x.test,apply(model$yhat.test,2,quantile,.975),col=1,type='l',lwd=1)
lines(x.test,apply(model$yhat.test,2,quantile,.025),col=1,type='l',lwd=1)
dev.off
dev.off
()
dev.off()
set.seed(301)
f=function(x){
ifelse(x<.2,pnorm(x),-2*x^2+2*x)
}
curve(f,-1,1)
n = 100
xx = runif(n,-1,1)
yy = f(xx) + rnorm(n,0,.04)
points(xx,yy,pch=19,cex=.1)
x.test = seq(-1,1,length.out = 1000)
x.test
model = wbart(x.train = xx,y.train = yy,x.test = x.test,ntree=200)
matplot(x.test,t(model$yhat.test),type='l',col=rgb(1,0,0,alpha=0.05),lty=1)
points(xx,yy,pch=19,cex=.8,col=rgb(0,0,0,alpha=0.5),xlab="X",ylab='Y')
lines(x.test,model$yhat.test.mean,col=1,type='l',lwd=2.5)
lines(x.test,apply(model$yhat.test,2,quantile,.975),col=1,type='l',lwd=1)
lines(x.test,apply(model$yhat.test,2,quantile,.025),col=1,type='l',lwd=1)
rm(list=ls())
library(mosaic)
##### Working with on original Levitt data
#' The response variable, Y , is per capita crime rates (violent crime, property crime, and murders) by state, from 1985 to 1997 (inclusive). The treatment variable, Z, is the “effective” abortion rate. This metric is an averaged abortion rate, weighted by criminal age at the time of arrest (to account for the fact that crimes committed by criminals should be associated with abortion rates at the time of their births).
##########################################
Original = read.table("../data/levitt_ex.dat",fill=TRUE,header=TRUE)
n=dim(Original)[1]
head(Original)
ind1 = (1:n)[Original$statenum==9]
ind2 = (1:n)[Original$statenum==2]
ind3 = (1:n)[Original$statenum==12]
ind = c(ind1,ind2,ind3)
ind1 = (1:n)[Original$year>97]
ind2 = (1:n)[Original$year<85]
ind = c(ind,ind1,ind2)
ind = unique(ind)
ind
n
Data = Original[-ind,]
ind = complete.cases(Data)
Data = Data[ind,]
Data[,2] = Data[,2]-84
dim(data)
dim(Data)
head(Data)
state.f = factor(Data$statenum)
states_dummies = model.matrix(~state.f)
year.f = factor(Data$year)
year_dummies = model.matrix(~year.f)
Y_M = Data$lpc_murd
D_M = Data$efamurd
D_M
Controls = Data[,-c(1:9)]
year = Data[,2]
Interactions = Controls*as.numeric(year)
Interactions2 = Controls*(as.numeric(year)^2)
Interactions3 = states_dummies*as.numeric(year)
Interactions4 = states_dummies*as.numeric(year^2)
XO = as.matrix(cbind(Controls,states_dummies[,2:48],year_dummies[,2:13]))
XO = scale(XO)
X = as.matrix(cbind(Controls,states_dummies[,2:48],year_dummies[,2:13],Interactions,Interactions2,Interactions3[,2:48],Interactions4[,2:48]))
X = scale(X)
X = X[,-71]
confint(lm(Y_V~D_V+XO))[2,]
Y_M = Data$lpc_murd
D_M = Data$efamurd
Y_P = Data$lpc_prop
D_P = Data$efaprop
Y_V = Data$lpc_viol
D_V = Data$efaviol
confint(lm(Y_V~D_V+XO))[2,]
confint(lm(Y_P~D_P+XO))[2,]
confint(lm(Y_M~D_M+XO))[2,]
head(Dat)
head(Data)
dev.off()
confint(lm(Y_V~D_V+XO))[2,]
confint(lm(Y_P~D_P+XO))[2,]
confint(lm(Y_M~D_M+XO))[2,]
CE_V_OLS = confint(lm(Y_V~D_V+X))[2,]
CE_V_OLS
CE_P_OLS = confint(lm(Y_P~D_P+X))[2,]
CE_M_OLS = confint(lm(Y_M~D_M+X))[2,]
CE_V_OLSsmallmodel = confint(lm(Y_V~D_V+XO))[2,]
## lasso-based selection (naive - large model)
library(glmnet)
boot = do(100)*{
rowindices = 1:nrow(X)
iixx = sample(rowindices,size=length(rowindices),replace=TRUE)
Xmod = X[iixx,]; Dmod = D_P[iixx]; Ymod = Y_V[iixx]
# fitting the model!
fit_reg = cv.glmnet(x=cbind(Dmod,Xmod),y=Ymod,family="gaussian",alpha=0,penalty.factor	=c(0,rep(1,ncol(Xmod))),nlambda=20)
as.numeric(coef(fit_reg,s="lambda.1se")[2])
}
causal_effect = unlist(boot)
hist(causal_effect)
CE_V_naive = confint(causal_effect,level = 0.95)
## lasso-based selection (the right way! - large model)
library(glmnet)
boot = do(100)*{
rowindices = 1:nrow(X)
iixx = sample(rowindices,size=length(rowindices),replace=TRUE)
Xmod = X[iixx,]; Dmod = D_P[iixx]; Ymod = Y_V[iixx]
# fitting the treatment model
fit_D = cv.glmnet(x=Xmod,y=Dmod,family="gaussian",alpha=0,nlambda=20)
Dhat = predict(fit_D,newx=Xmod,s="lambda.1se")
# fitting the outcome model
fit_reg = cv.glmnet(x=cbind(Dmod-Dhat,Xmod),y=Ymod,family="gaussian",alpha=0,penalty.factor	=c(0,rep(1,ncol(Xmod))),nlambda=20)
as.numeric(coef(fit_reg,s="lambda.1se")[2])
}
causal_effect = unlist(boot)
hist(causal_effect)
CE_V_best = confint(causal_effect,level = 0.95)
CE_V_OLSsmallmodel
CE_V_best
CE_V_OLS
CE_V_naive
# plot to visualize
yval = c(1,1,2,2,3,3,4,4)
xval = unlist(c(CE_V_OLSsmallmodel,CE_V_OLS,CE_V_naive,CE_V_best))
plot(xval,yval,col='white',bty='n',xlab='causal effect',main='Effect of abortion rate on violent crime')
legend('topleft',legend=c('OLS-small','OLS-large','Reg-naive','Reg-best'),col=c('red','blue','orange','green'),lwd=4)
abline(v=0,lty=2)
lines(xval[1:2],yval[1:2],lwd=10,col='red')
lines(xval[3:4],yval[3:4],lwd=10,col='blue')
lines(xval[5:6],yval[5:6],lwd=10,col='orange')
lines(xval[7:8],yval[7:8],lwd=10,col='green')
dev.off()
rm(list=ls())
library(mosaic)
##### Working with on original Levitt data
#' The response variable, Y , is per capita crime rates (violent crime, property crime, and murders) by state, from 1985 to 1997 (inclusive). The treatment variable, Z, is the “effective” abortion rate. This metric is an averaged abortion rate, weighted by criminal age at the time of arrest (to account for the fact that crimes committed by criminals should be associated with abortion rates at the time of their births).
##########################################
Original = read.table("../data/levitt_ex.dat",fill=TRUE,header=TRUE)
n=dim(Original)[1]
rm(list=ls())
library(mosaic)
##### Working with on original Levitt data
#' The response variable, Y , is per capita crime rates (violent crime, property crime, and murders) by state, from 1985 to 1997 (inclusive). The treatment variable, Z, is the “effective” abortion rate. This metric is an averaged abortion rate, weighted by criminal age at the time of arrest (to account for the fact that crimes committed by criminals should be associated with abortion rates at the time of their births).
##########################################
Original = read.table("../data/levitt_ex.dat",fill=TRUE,header=TRUE)
n=dim(Original)[1]
## Remove DC, Alaska and Hawaii
ind1 = (1:n)[Original$statenum==9]
ind2 = (1:n)[Original$statenum==2]
ind3 = (1:n)[Original$statenum==12]
ind = c(ind1,ind2,ind3)
ind1 = (1:n)[Original$year>97]
ind2 = (1:n)[Original$year<85]
ind = c(ind,ind1,ind2)
ind = unique(ind)
Data = Original[-ind,]
ind = complete.cases(Data)
Data = Data[ind,]
Data[,2] = Data[,2]-84
head(Data)
state.f = factor(Data$statenum)
states_dummies = model.matrix(~state.f)
year.f = factor(Data$year)
year_dummies = model.matrix(~year.f)
Y_M = Data$lpc_murd
D_M = Data$efamurd
Y_P = Data$lpc_prop
D_P = Data$efaprop
Y_V = Data$lpc_viol
D_V = Data$efaviol
Controls = Data[,-c(1:9)]
year = Data[,2]
Interactions = Controls*as.numeric(year)
Interactions2 = Controls*(as.numeric(year)^2)
Interactions3 = states_dummies*as.numeric(year)
Interactions4 = states_dummies*as.numeric(year^2)
XO = as.matrix(cbind(Controls,states_dummies[,2:48],year_dummies[,2:13]))
XO = scale(XO)
X = as.matrix(cbind(Controls,states_dummies[,2:48],year_dummies[,2:13],Interactions,Interactions2,Interactions3[,2:48],Interactions4[,2:48]))
X = scale(X)
X = X[,-71]
confint(lm(Y_V~D_V+XO))[2,]
confint(lm(Y_P~D_P+XO))[2,]
confint(lm(Y_M~D_M+XO))[2,]
dim(X0)
dim(XO)
dim(X)
CE_V_OLS = confint(lm(Y_V~D_V+X))[2,]
CE_P_OLS = confint(lm(Y_P~D_P+X))[2,]
CE_M_OLS = confint(lm(Y_M~D_M+X))[2,]
CE_V_OLS
CE_P_OLS
CE_M_OLS
CE_V_OLSsmallmodel = confint(lm(Y_V~D_V+XO))[2,]
library(glmnet)
boot = do(100)*{
rowindices = 1:nrow(X)
iixx = sample(rowindices,size=length(rowindices),replace=TRUE)
Xmod = X[iixx,]; Dmod = D_P[iixx]; Ymod = Y_V[iixx]
# fitting the model!
fit_reg = cv.glmnet(x=cbind(Dmod,Xmod),y=Ymod,family="gaussian",alpha=0,penalty.factor	=c(0,rep(1,ncol(Xmod))),nlambda=20)
as.numeric(coef(fit_reg,s="lambda.1se")[2])
}
causal_effect = unlist(boot)
hist(causal_effect)
CE_V_naive = confint(causal_effect,level = 0.95)
library(glmnet)
boot = do(100)*{
rowindices = 1:nrow(X)
iixx = sample(rowindices,size=length(rowindices),replace=TRUE)
Xmod = X[iixx,]; Dmod = D_P[iixx]; Ymod = Y_V[iixx]
# fitting the treatment model
fit_D = cv.glmnet(x=Xmod,y=Dmod,family="gaussian",alpha=0,nlambda=20)
Dhat = predict(fit_D,newx=Xmod,s="lambda.1se")
# fitting the outcome model
fit_reg = cv.glmnet(x=cbind(Dmod-Dhat,Xmod),y=Ymod,family="gaussian",alpha=0,penalty.factor	=c(0,rep(1,ncol(Xmod))),nlambda=20)
as.numeric(coef(fit_reg,s="lambda.1se")[2])
}
causal_effect = unlist(boot)
hist(causal_effect)
CE_V_best = confint(causal_effect,level = 0.95)
# plot to visualize
yval = c(1,1,2,2,3,3,4,4)
xval = unlist(c(CE_V_OLSsmallmodel,CE_V_OLS,CE_V_naive,CE_V_best))
plot(xval,yval,col='white',bty='n',xlab='causal effect',main='Effect of abortion rate on violent crime')
legend('topleft',legend=c('OLS-small','OLS-large','Reg-naive','Reg-best'),col=c('red','blue','orange','green'),lwd=4)
plot(xval,yval,col='white',bty='n',xlab='causal effect',main='Effect of abortion rate on violent crime')
legend('topleft',legend=c('OLS-small','OLS-large','Reg-naive','Reg-best'),col=c('red','blue','orange','green'),lwd=4)
abline(v=0,lty=2)
lines(xval[1:2],yval[1:2],lwd=10,col='red')
lines(xval[3:4],yval[3:4],lwd=10,col='blue')
lines(xval[5:6],yval[5:6],lwd=10,col='orange')
lines(xval[7:8],yval[7:8],lwd=10,col='green')
students = read.csv("../data/students_2022.csv",header = T)
students
students = read.csv("../data/students_2022.csv",header = F)
students
ord = sample(1:nrow(students))
ord
## project presentation script
set.seed(1)
students = read.csv("../data/students_2022.csv",header = F)
ord = sample(1:nrow(students))
final = cbind(rep('JPE',6),rep('Walmart',6),students[ord])
students
students[ord]
students = as.character(read.csv("../data/students_2022.csv",header = F))
ord = sample(1:nrow(students))
ord = sample(1:length(students))
final = cbind(rep('JPE',6),rep('Walmart',6),students[ord])
colnames(final) = ''
final
students[ord]
students = as.string(read.csv("../data/students_2022.csv",header = F))
students
students = read.csv("../data/students_2022.csv",header = F)
ord = sample(1:nrow(students))
final = cbind(rep('JPE',6),rep('Walmart',6),students[ord,])
final
set.seed(1)
students = read.csv("../data/students_2022.csv",header = F)
ord = sample(1:nrow(students))
final = cbind(noquote(c(rep('JPE',6),rep('Walmart',6))),students[ord,])
final
## project presentation script
set.seed(1)
students = read.csv("../data/students_2022.csv",header = F)
ord = sample(1:nrow(students))
final = cbind(noquote(c(rep('JPE',6),rep('Walmart',6))),students[ord,])
final
noquote(final)
dev.off()
