for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
library(MASS)
library(FNN)
set.seed(1)
N = 1000
K = 5
### 70 percentage of schools were assigned to 6ft, i.e. 1: 6ft, 0: 4ft
X1 <- rbinom(n = N, size = 1, prob = 0.7)
### Spatial Location: those with 6ft policy have spatial locations between 0 and 1 for both coordinates and those with 4ft policy have spatial locations between 0.3 and 0.6 for both coordinates
U1 <- runif(n = N, 0.3, 0.6)*(1-X1)
U2 <- runif(n = N, 0.3, 0.6)*(1-X1)
U1 <- U1 + runif(n = N, 0, 1)*X1
U2 <- U2 + runif(n = N, 0, 1)*X1
U <- cbind(U1, U2)
X2 <- rep(NA, N)
KNN <- get.knn(U, k = K)
ind <- KNN$nn.ind ### Contains index of 10 closest elements
for(i in 1:N){
X2[i] <- sum(X1[ind[i,]])/K
### X2 is the proportion of schools around which had 6ft policy
}
X <- cbind(X1, X2)
Y <- X%*%c(1, -1.2) + rnorm(n = N, 0, 1)
cov(Y, X[,1]) ### Illustrates the negative covariance
cov(X[,1], X[,2]) ### Illustrates the positive covariance
install.packages("devtools")
library(devtools)
install_github("dpuelz/BicliqueRT")
library(devtools)
install_github("dpuelz/BicliqueRT")
library(BicliqueRT)
hello
blah
install.packages("swirl")
library(swirl)
install_course_github("kosukeimai","qss-swirl")
library()
swirl()
install_course("qss-swirl")
install_course("qss-swirl")
install_course("qss-swirl")
library(swirl) # load the swirl package
install_course_github("kosukeimai", "qss-swirl")
swirl()
8-2
10^2
sqrt(9)
remotes::install_github("kosukeimai/qss-package", build_vignettes = TRUE)
test <- function(seed){
res <- seed + seed
}
test(1)
test(1)
test <- function(seed){
res <- seed + seed
}
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
##
test <- function(seed){
res <- seed + seed
res
}
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
test(1)
x=c(1,2,3,4)
y=c(1,2,3,4)
table(x~y)
table(x)
summary(table)
as.table(x,y)
?Summarize
load('Downloads/tobs_debug.rds')
readRDS('Downloads/tobs_debug.rds')
readRDS('Downloads/tval_debug.rds')
readRDS('Downloads/pvalue_debug.rds')
## ------------------------------------------------------------------------
# Let's consider an experiment where researchers have a set of resumes, and then change the names at the top to white and black sounding names.  There are 36 different names in total.  4870 total observations,
resume <- read.csv("../data/resume.csv")
setwd("~/Dropbox/SalemCenter/classes/PRL/Fall2022/Policy-Research-Laboratory/code")
## ------------------------------------------------------------------------
# Let's consider an experiment where researchers have a set of resumes, and then change the names at the top to white and black sounding names.  There are 36 different names in total.  4870 total observations,
resume <- read.csv("../data/resume.csv")
dim(resume)
head(resume)
summary(resume)
## ------------------------------------------------------------------------
# Let's consider an experiment where researchers have a set of resumes, and then change the names at the top to white and black sounding names.  There are 36 different names in total.  4870 total observations,
resume <- read.csv("../data/resume.csv")
dim(resume)
head(resume)
summary(resume)
race.call.tab <- table(race = resume$race, call = resume$call)
race.call.tab
setwd("~/Dropbox/SalemCenter/classes/PRL/Fall2022/Policy-Research-Laboratory/code")
setwd("~/Dropbox/SalemCenter/classes/PRL/Fall2022/Policy-Research-Laboratory/code")
resume <- read.csv("../data/resume.csv")
resume <- read.csv("../data/resume.csv")
dim(resume)
head(resume)
summary(resume)
race.call.tab <- table(race = resume$race, call = resume$call)
race.call.tab
addmargins(race.call.tab)
sum(race.call.tab[, 2]) / nrow(resume)
dim(resume)[1]
race.call.tab[, 2]
sum(race.call.tab[, 2]) / nrow(resume)
race.call.tab[1, 2]
race.call.tab
race.call.tab[1, ]
sum(race.call.tab[1, ])
race.call.tab[1, 2]
race.call.tab[1, 2] / sum(race.call.tab[1, ]) # black
race.call.tab[2, 2] / sum(race.call.tab[2, ]) # white
race.call.tab[1, ]  # the first row
race.call.tab[ , 2]  # the second column
race.call.tab[,2]  # the second column
mean(resume$call) # same as line 15!
class(TRUE)
as.integer(TRUE)
as.integer(FALSE)
x <- c(TRUE, FALSE, TRUE) # a vector with logical values
x
mean(x)
sum(x) # number of TRUEs
FALSE & TRUE
TRUE & TRUE
TRUE | FALSE
FALSE | FALSE
TRUE & FALSE & TRUE
TF1 <- c(TRUE, FALSE, FALSE)
TF2 <- c(TRUE, FALSE, TRUE)
TF1
TF2
TF1 | TF2
4 < 3
"Hello" == "hello"  # R is case-sensitive
"Hello" != "hello"
charles = 'student'
charles <- 'student'
charles <- 'beststudent'
1:100
test = 1:100
test
testsq = test^2
testsq
plot(testsq)
resume$race
resume$race == "black"
resume$call[resume$race == "black"]
mean(resume$call[resume$race == "black"])
resume$race[1:5]
(resume$race == "black")[1:5]
resumeB <- resume[resume$race == "black", ]
dim(resumeB)
mean(resumeB$call) # callback rate for blacks
resumeW <- resume[resume$race == "white", ]
dim(resumeW) # this data.frame has fewer rows than the original
mean(resumeW$call)
resumeBf <- subset(resume, select = c("call", "firstname"),
subset = (race == "black" & sex == "female"))
head(resumeBf)
dim(resumeBf)
resumeBm <- subset(resume, subset = (race == "black") & (sex == "male"))
resumeWf <- subset(resume, subset = (race == "white") & (sex == "female"))
resumeWm <- subset(resume, subset = (race == "white") & (sex == "male"))
mean(resumeWf$call) - mean(resumeBf$call) # among females
mean(resumeWm$call) - mean(resumeBm$call) # among males
heaD(resume)
head(resume)
#' In this script, we investigate a policing experiment and see how to use the
#' FRT to characterize statistical uncertainty in the DiM estimator of the SATE
# First, load in all of the data
load('../data/Z_police.RData') # treatment assignments
load('../data/network_police.RData') # network
load('../data/outcomes_police.RData') # outcomes of interest
# convert to data frames and reorder rows
network = as.data.frame(network)
dim(network)
head(network)
# convert to data frames and reorder rows
network = as.data.frame(network)
network = network[order(as.numeric(rownames(network))),]
outcomes = as.data.frame(outcomes)
outcomes = outcomes[order(as.numeric(rownames(outcomes))),]
head(outcomes)
head(network)
plot(network,xlab='',ylab='',pch=19,cex=0.4,col=rgb(0.5,0.5,0.5,alpha=0.5),xaxt='n',yaxt='n',bty='n')
head(outcomes)
Y = (outcomes$hmotos +  outcomes$hcarros)*0.221 +
outcomes$homicidio*0.550 + outcomes$lesiones*0.112 +
outcomes$hpersonas*0.116
names(Y) = rownames(outcomes)
points(network,col=rgb(1,0,0,alpha=0.5),cex=Y,pch=19)
dim(Z)
Zobs = Z[,1]; names(Zobs) = rownames(Z)
Zobs_long = rep(0,nrow(network)) # need to make a long Z
hotspot_indices = match(names(Zobs),rownames(network)) # indices in network dataframe that correspond to hotspot indices
Zobs_long[hotspot_indices] <- Zobs
plot(network,xlab='',ylab='',pch=19,cex=0.4,col=rgb(0.5,0.5,0.5,alpha=0.5),xaxt='n',yaxt='n',bty='n')
points(network[hotspot_indices,],col='blue',pch=19,cex=0.6)
points(network[which(Zobs_long==1),],col='green',pch=8,cex=0.75)
treatmentlong = function(Zalt,network,hotspot_indices){
Zalt
Zalt_long = rep(0,nrow(network))
Zalt_long[hotspot_indices] <- Zalt
return(Zalt_long)
}
# A second function that uses the one above for plotting
plottreatment = function(Zalt,network,hotspot_indices){
Zalt_long = treatmentlong(Zalt,network,hotspot_indices)
plot(network,xlab='',ylab='',pch=19,cex=0.4,col=rgb(0.5,0.5,0.5,alpha=0.5),xaxt='n',yaxt='n',bty='n')
points(network[hotspot_indices,],col='blue',pch=19,cex=0.6)
points(network[which(Zalt_long==1),],col='green',pch=8,cex=0.75)
}
# bringing it all together
plottreatment(Z[,1],network,hotspot_indices)
plottreatment(Z[,2],network,hotspot_indices)
plottreatment(Z[,3],network,hotspot_indices)
plottreatment(Z[,4],network,hotspot_indices)
plottreatment(Z[,5],network,hotspot_indices)
Yobs = Y[hotspot_indices]
DiM = mean(Yobs[which(Zobs==1)]) - mean(Yobs[which(Zobs==0)])
DiM
DiM_function = function(Zalt,Y){
DiM = mean(Y[which(Zalt==1)]) - mean(Y[which(Zalt==0)])
return(DiM)
}
DiM_observed = DiM_function(Z[,1],Yobs)
DiM_observed
# calculate alternative DiM's in a for loop
DiM_alt = c()
for(i in 1:ncol(Z[,-1])){
DiM_alt = c(DiM_alt,DiM_function(Z[,i+1],Yobs))
}
DiM_alt = apply(Z[,-1],MARGIN=2,DiM_function,Y=Yobs)
hist(DiM_alt)
abline(v=DiM_observed,col='blue',lwd=3)
p = mean(DiM_alt<DiM_observed)
min(p,1-p)
# homicides
Yobs = outcomes$homicidio[hotspot_indices]
DiM_alt = apply(Z[,-1],2,DiM_function,Y=Yobs)
DiM_observed = DiM_function(Z[,1],Yobs)
hist(DiM_alt)
abline(v=DiM_observed,col='blue',lwd=3)
p = mean(DiM_alt<DiM_observed)
min(p,1-p)
# car jackings
Yobs = outcomes$hcarros[hotspot_indices]
DiM_alt = apply(Z[,-1],2,DiM_function,Y=Yobs)
DiM_observed = DiM_function(Z[,1],Yobs)
hist(DiM_alt)
abline(v=DiM_observed,col='blue',lwd=3)
p = mean(DiM_alt<DiM_observed)
min(p,1-p)
# Example 1: binomial for airline no shows
# Parameters
P = 0.09
N = 140
# Create a grid of k's
k_grid = seq(0, 30, by=1)
# Probability mass function of binomial
dbinom(k_grid, N, P)
barplot(dbinom(k_grid, N, P), names.arg = k_grid,
xlab='Number of no shows',
ylab='Probability')
# Left tail area
sum(dbinom(0:5, N, P))
sum(dbinom(12:140, N, P)) # this is the answer to our question on the slides!
1 - pbinom(11, size=N, prob=P) # using the cumulative distribution function instead
# Example 2: Poisson for soccer scores
x_grid = 0:7
dpois(x_grid, 1.6)
barplot(dpois(x_grid, 1.6), names.arg = x_grid,
cex.names=1.1, las=1, ylim=c(0,0.35),
xlab='Goals',
ylab='Probability', main="UT: Poisson(1.6)")
barplot(dpois(x_grid, 1.3), names.arg = x_grid,
cex.names=1.1, las=1, ylim=c(0,0.35),
xlab='Goals',
ylab='Probability', main="Texas Tech: Poisson(1.3)")
# Simulate lots of games according to these Poissons
NMC = 100000
UT = rpois(NMC, 1.6)
Tech = rpois(NMC, 1.3)
# Compile the results
xtabs(~UT + Tech)
results_table = xtabs(~UT + Tech)/NMC
results_table
# Monte Carlo estimates of probabilities
sum(UT > Tech)/NMC
sum(UT == Tech)/NMC
sum(UT < Tech)/NMC
dpois(1, 1.6) * dpois(1, 1.3) # UT 1 - 1 Texas Tech
dpois(2, 1.6) * dpois(0, 1.3) # UT 2 - 0 Texas Tech
# A cool figure for those who want to geek out with R graphics
my_cols = grey(seq(1, 0.5, length=15))
my_breaks = seq(0, 0.12, length=16)
image(0:5, 0:5, results_table[1:5,1:5],
xlim=c(-0.5, 5.5),
ylim=c(-0.5, 5.5),
col=my_cols, breaks=my_breaks,
las=1, bty='n',
main="Probability of outcomes for the \nUT vs. Texas Tech match",
ylab='UT goals',
xlab="Texas Tech goals")
abline(h=-0.5 + 0:6, lty='dotted')
abline(v=-0.5 + 0:6, lty='dotted')
for(i in 1:6) {
for(j in 1:6) {
text(i-1, j-1, results_table[i,j])
if(i > j) {
rect(i-1.5,j-1.5,i-0.5, j-0.5, border='blue', lwd=2)
}
}
}
# Example 3: Functions for continuous variables
# normal random variables
hist(rnorm(1000,mean=0,sd=1),col='gray',freq = FALSE)
K = 3
pnorm(K,mean=0,sd=1) # cumulative density function, calculate P(x<=K) for one value of K
# show PDF and CDF next to each other
ii = seq(-3,3,length.out=1000)
plot(ii,pnorm(ii,mean=0,sd=1),type='l',xlab='')
par(mfrow=c(1,2))
hist(rnorm(10000,mean=0,sd=1),col='gray',freq = FALSE,xlab='',main='PDF of Normal')
plot(ii,pnorm(ii,mean=0,sd=1),type='l',xlab='',main='CDF of Normal')
X = runif(1000)
hist(X)
hist_data <- hist(X)
counts = hist_data$counts
breaks = hist_data$breaks
breaks_new = breaks[-length(breaks)]  + (breaks[-1] - breaks[-length(breaks)])/2
lines(breaks_new,counts,lwd=3,lty=2,col='red')
abline(v=0.8,col=2,lwd=3)
# example of adding a line on top of a histogram
dev.off()
